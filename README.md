# DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs

<a href="https://arxiv.org/abs/2402.03898"><img src="https://img.shields.io/badge/Paper-arXiv:2402.03898-Green"></a>
<a href=#bibtex><img src="https://img.shields.io/badge/Paper-BibTex-yellow"></a>

Official PyTorch implementation of **DistiLLM-2**, as presented in our paper: \
\
**DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs** \
*[Jongwoo Ko](https://sites.google.com/view/jongwooko), Tianyi Chen, Sungnyun Kim, Tianyu Ding, Luming Liang, Ilya Zharkov, Se-Young Yun* \
KAIST AI and Microsoft

## Acknowledgement
The preliminary code is available in our previous [repo](https://github.com/jongwooko/distillm). The final version of the code for this paper will be available soon.

## BibTeX
If you find this repo useful for your research, please consider citing our paper:

```
@article{ko2024distillm,
      title={DistiLLM: Towards Streamlined Distillation for Large Language Models}, 
      author={Jongwoo Ko and Sungnyun Kim and Tianyi Chen and Se-Young Yun},
      year={2024},
      journal={arXiv preprint arXiv:2402.03898},
}
```

## Contact
- Jongwoo Ko: jongwoo.ko@kaist.ac.kr